{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 1. Setup and Initialization\n",
    "from groq import Groq\n",
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "client = Groq(api_key = API_KEY)\n",
    "model = 'whisper-large-v3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. Audio Transcription\n",
    "def audio_to_text(filepath):\n",
    "    with open(filepath,'rb') as file:\n",
    "        transalation = client.audio.translations.create(\n",
    "            file = (filepath, file.read()),\n",
    "            model = 'whisper-large-v3'\n",
    "            \n",
    "        )\n",
    "    return transalation.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"Groq's AI Chip Breaks Speed Records.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Welcome back, you're with Connect The World. I'm Becky Anderson. We're at the World Government Summit in Dubai and one thing I've noticed here is that whenever a discussion about artificial intelligence takes place, the rooms here, the huge halls get packed. That is because some of the leading minds behind the technological revolution have been gathered here at the conference over the past couple of days and in AI's race to the top my next guest is sprinting at speeds never seen before Jonathan Ross is the brain behind GROK the world's first language processing unit now before I lose you in the technological jargon of AI let me put it this way what Ross created is a chip that can run programs like Meta's Llama 2 model for example faster than anything else in the world. 10 to 100 times faster in fact and he's here with me now to explain how that is possible. Before I ask you that, Grok, why Grok? Thank you Becky. It's Grok and we spell it with a Q and it's because it comes from a science fiction novel and it means to understand something deeply and with empathy. Of course it Tell us about your chip and what makes Brock Chip LPU different from other AI chips and accelerators. I have to tell our viewers that the NVIDIA CEO was here, of course, this week at the beginning of the week. So we've had all the greatest minds in here. What's your story? Well, asking me how the chip works before I show you what it does is a bit like asking how a magic trick works before showing you the magic trick. You're going to get bored, but I'll give it a shot. Cool. So most chips they don have enough memory inside of them Sort of like if you were building cars and you use a giant factory and you need about a million square feet of assembly line space Well if you don have a building that large enough to fit that, then you need to set up part of the assembly line, tear it down over and over again. Right. And that's slow and it takes a lot of time. And that's what happens with the GPU. You \n"
     ]
    }
   ],
   "source": [
    "translation_text = audio_to_text(filepath)\n",
    "print(translation_text[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The summit is organized in Dubai.\n"
     ]
    }
   ],
   "source": [
    "# 3. Transcript Chat Completion\n",
    "def transcript_chat_completion(client, transcript, user_question):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": '''Use this transcript or transcripts to answer any user questions, citing specific quotes:\n",
    "\n",
    "                {transcript}\n",
    "                '''.format(transcript=transcript)\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_question,\n",
    "            }\n",
    "        ],\n",
    "        model=\"llama3-8b-8192\",\n",
    "    )\n",
    "    print(chat_completion.choices[0].message.content)\n",
    "\n",
    "user_question = \"In which city this summit is organized\"\n",
    "transcript_chat_completion(client, translation_text, user_question)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
